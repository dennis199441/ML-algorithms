{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is a field of computer science that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "For example, given a dataset of {X, y} and let the underlying relationship of X and y be y = f(X). Using machine learning can find out the underlying function f and use f to make the decision.\n",
    "\n",
    "To find out the underlying f, probability and statistics are essential. Therefore, data plays an important role in machine learning. \n",
    "\n",
    "To make good use of data, cross validation is done usually. We split our dataset into two parts, training set and testing set. Using the training set, we can generate a model and optimise its parameter(s) and using the testing set to test the generated model and compute the accuracy of the model.\n",
    "\n",
    "There are three type of learning: Supervised Learning, Unsupervised Learning and Reinforcement Learning. In this notebook, we are going to focus on supervised and unsupervised learning and build the algorithms from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are wide range of applications of machine learning. There are a lot of classification and regression problems in real life. \n",
    "\n",
    "For example, spam email detection is a typical classification problem. Given some features of spam email, we can create a model to classify whether it is a spam or not. Another example is to predict the future for a given time series data such as stock price. Using the historical data like trading volumn, volatility and other technical indicators, we might be able to predict the future price and machine learning can be applied to find this hidden relationship.\n",
    "\n",
    "Not only can we solve the classification/regression problems using a labelled dataset, we can also extract information from images and videos, which labels are not provided. These technologies are called image processing and video processing respectively. Unsupervised machine learning model like K-Mean Clustering can be used. \n",
    "\n",
    "The main use of machine learning is to recognise the pattern of the data and predict the next and help us to make the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is the task of inferring a function from labeled training data. Given X and y, find out the function f. \n",
    "\n",
    "Supervised Learning problems can be grouped into regression and classification problems.\n",
    "\n",
    "Classification: A classification problem is when the output variable is a category, such as \"red\" and \"blue\".\n",
    "\n",
    "Regression: A regression problem is when the output variable is real value, such as \"dollars\" or \"weight\".\n",
    "\n",
    "The following are the examples of supervised learning algorithms:\n",
    "1. Linear Regression for regression.\n",
    "2. Decision Trees for classification.\n",
    "3. Random Forest for both classification and regression.\n",
    "4. Support Vector Machines for both classification and regression.\n",
    "5. k-Nearest Neighbors for both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea Behind Simple Linear Regression:\n",
    "\n",
    "Minimize the squared error!\n",
    "\n",
    "let y_hat_i = b + M * x_hat_i\n",
    "\n",
    "squared-error = (y_hat_i - b - M * x_hat_i)^2\n",
    "\n",
    "sum-of-squared-errors (E) = summation((y_hat_i - b - M * x_hat_i)^2)\n",
    "\n",
    "To minumize the sum of squared errors, we need to solve the following equations.\n",
    "    dE/db = -2*summation(y_hat_i - b - M * x_hat_i) = 0\n",
    "\n",
    "    dE/db = summation(y_hat_i) - n * b - M * summation(x_hat_i) = 0\n",
    "\n",
    "        summation(y_hat_i) = M * summation(x_hat_i) + n * b\n",
    "\n",
    "        b = (summation(y_hat_i) - M * summation(x_hat_i))/ n ------(Equation 1)\n",
    "\n",
    "\n",
    "    dE/dM = -2*summation(x_hat_i * (y_hat_i - b - M* x_hat_i)) = 0\n",
    "\n",
    "    dE/dM = summation(x_hat_i * y_hat_i - b * x_hat_i - M * (x_hat_i)^2)\n",
    "\n",
    "    dE/dM = summation(x_hat_i * y_hat_i) - b * summation(x_hat_i) - M * summation((x_hat_i)^2) = 0 ------(Equation 2)\n",
    "\n",
    "put Equation 1 into Equation 2\n",
    "    summation(x_hat_i * y_hat_i) - b * summation(x_hat_i) - M * summation((x_hat_i)^2) = 0\n",
    "\n",
    "    summation(x_hat_i * y_hat_i) = M * summation((x_hat_i)^2) + b * summation(x_hat_i)\n",
    "\n",
    "    summation(x_hat_i * y_hat_i) = M * summation((x_hat_i)^2) + (summation(y_hat_i) - M * summation(x_hat_i))/ n *  summation(x_hat_i)\n",
    "\n",
    "    summation(x_hat_i * y_hat_i) = M * summation((x_hat_i)^2) + (summation(y_hat_i) * summation(x_hat_i))/n - M *  (summation(x_hat_i) * summation(x_hat_i))/ n\n",
    "\n",
    "    summation(x_hat_i * y_hat_i) - (summation(y_hat_i) * summation(x_hat_i))/n  = M * summation((x_hat_i)^2) -  (summation(x_hat_i) * summation(x_hat_i))/ n\n",
    "\n",
    "M = (n * summation(x_hat_i * y_hat_i) - summation(y_hat_i) * summation(x_hat_i))/\n",
    "    (n* summation((x_hat_i)^2) - summation(x_hat_i)^2)\n",
    "  \n",
    "\n",
    "From Equation 1: b = mean(y) - M mean(x)\n",
    "\n",
    "\n",
    "M = (-1/n^2)/(-1/n^2) * M\n",
    "\n",
    "\n",
    "M = -(mean(x*y) - mean(y) * mean(x)) / -(mean(x^2) - mean(x) * mean(x))\n",
    "\n",
    "\n",
    "M = (mean(y) * mean(x) - mean(x*y)) /\n",
    "    (mean(x) * mean(x) - mean(x^2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LinearRegression.py\n",
    "from statistics import mean\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    ## y = M * X + b\n",
    "    ## fit() minimize the squared error \n",
    "    def fit(self, X, y):\n",
    "        M = ( (mean(X) * mean(y) - mean(X * y)) /\n",
    "            (mean(X) * mean(X) - mean(X * X)) )\n",
    "\n",
    "        b = mean(y) - M * mean(X)\n",
    "\n",
    "        self.M = M\n",
    "        self.b = b\n",
    "\n",
    "    def R_squared(self, y, y_hat):\n",
    "        y_mean = [mean(y) for data in y]\n",
    "        ## sum of squares of residuals (RSS)\n",
    "        RSS = sum((y_hat - y)**2)\n",
    "        ## total sum of squares (TSS)\n",
    "        TSS = sum((y_mean - y)**2)\n",
    "\n",
    "        ## R^2 = 1 - (RSS/TSS) = ESS/TSS\n",
    "        return 1 - (RSS / TSS)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for data in X:\n",
    "            predicted.append(self.M * data + self.b)\n",
    "        \n",
    "        return predicted\n",
    "\n",
    "    def regression_line(self, X):\n",
    "        return [(self.M * data) + self.b for data in X]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is similar to Simple Linear Regression. The only difference is that multiple linear regression model contains more independent variables. \n",
    "\n",
    "For example, y1 = b0 + b1 * x1 + b2 * x2 + b3 * x3 + .... + bn * xn\n",
    "\n",
    "Parameters: [b0, b1, ..., bn] are optimized using gradient descent.\n",
    "\n",
    "Suppose we are at the top of a hill and we want to reach the bottom of the hill (minimum point). We are heading down step by step.  In this case, we need a magnitude and direction. These two parameters are calculated using error function. In linear regression, we use the sum of differences between predicted y and actual y as the error function and we want to minimize it.\n",
    "\n",
    "Since we are using gradient descent, we need to define the number of epoch (n_epoch) and the learning rate.\n",
    "\n",
    "n_epoch defines how many times do we need to loop through our dataset. For example, n_epoch = 10, we loop through our training data 10 times during the fitting process.\n",
    "\n",
    "Learning rate defines how large the steps will be. If the learning rate is too large, we might miss the minimum point. If the learning rate is too small, it may take a long time to minimize the error function.\n",
    "\n",
    "For each epoch, coefficients are updated n time if the length of the dataset is n according to the following logic.\n",
    "\n",
    "new b_0 = old b_0 - learning rate * error\n",
    "\n",
    "new b_i = old b_i - learning rate * error * feature_i (where i != 0)\n",
    "\n",
    "After sufficient number of epoch with an appropiate learning rate, sum of error will converge and reach the local minimum point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultipleLinearRegression:\n",
    "\n",
    "    def __init__(self, learning_rate, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "\n",
    "    def predict(self, features):\n",
    "        y_hat = self.coef[0]\n",
    "        for i in range(self.n_features):\n",
    "            y_hat += self.coef[i+1] * features[i]\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_features = len(X[0])\n",
    "\n",
    "        self.coef = [0.0 for i in range(self.n_features + 1)]\n",
    "\n",
    "        for epoch in range(self.n_epoch):\n",
    "            print(\"Epoh: %s\" % epoch)\n",
    "            print(\"Coefficient: %s\" % self.coef)\n",
    "            for i in range(len(X)):\n",
    "                y_hat = self.predict(X[i])\n",
    "                error = y_hat - y[i][0]\n",
    "                self.coef[0] = self.coef[0] - self.learning_rate * error\n",
    "                for j in range(self.n_features):\n",
    "                    self.coef[j+1] = self.coef[j+1] - self.learning_rate * error * X[i][j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree divides the labelled dataset into smaller subsets by \"asking\" different questions.\n",
    "In order to find out what questions can be ask and which question should we ask, \"Information Gain\" should be computed. For example, if the feature (color) is red, the label must be \"Apple\". If we ask if the color is red, we can directly find out which group the object should belong to.\n",
    "\n",
    "To calculate the Information Gain, gini impurity is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DecisionTree.py\n",
    "class DecisionTree:\n",
    "\n",
    "    ## Find the unique values for a column in dataset\n",
    "    def unique_vals(self, rows, col):\n",
    "        return set([row[col] for row in rows])\n",
    "\n",
    "    ## Count the number of each type of example in dataset\n",
    "    def class_counts(self, rows):\n",
    "        counts = {}\n",
    "        for row in rows:\n",
    "            label = row[-1]\n",
    "            if label not in counts:\n",
    "                counts[label] = 0\n",
    "\n",
    "            counts[label] += 1\n",
    "\n",
    "        return counts\n",
    "\n",
    "    ## Test if value is numeric\n",
    "    def isNumeric(self, value):\n",
    "        return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "    def partition(self, rows, question):\n",
    "        ## Partition the dataset.\n",
    "        ## For each row in the dataset, check if it matches the question. \n",
    "        ## If yes, add it to 'true rows', otherwise, add to 'false rows' \n",
    "        true_rows = []\n",
    "        false_rows = []\n",
    "        for row in rows:\n",
    "            if question.match(row):\n",
    "                true_rows.append(row)\n",
    "            else:\n",
    "                false_rows.append(row)\n",
    "\n",
    "        return true_rows, false_rows\n",
    "\n",
    "    def gini(self, rows):\n",
    "        ## Calculate the Gini Inpurity for a list of rows.\n",
    "        counts = self.class_counts(rows)\n",
    "        impurity = 1\n",
    "        for lbl in counts:\n",
    "            prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "            impurity -= prob_of_lbl**2\n",
    "\n",
    "        return impurity\n",
    "\n",
    "    def info_gain(self, left, right, current_uncertainty):\n",
    "        ## Information Gain.\n",
    "        ## The uncertainty of the starting node, minus the weighted impurity of\n",
    "        ## two child nodes.\n",
    "        p = float(len(left)) / (len(left) + len(right))\n",
    "\n",
    "        return current_uncertainty - p * self.gini(left) - (1 - p) * self.gini(right)\n",
    "\n",
    "    def find_best_split(self, rows):\n",
    "        ## Find the best question to ask by iterating over every feature / value\n",
    "        ## and calculating the Information Gain\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        current_uncertainty = self.gini(rows)\n",
    "        n_features = len(rows[0]) - 1\n",
    "\n",
    "        for col in range(n_features):\n",
    "            values = set([row[col] for row in rows])\n",
    "            for val in values:\n",
    "                question = Question(col, val)\n",
    "                true_rows, false_rows = self.partition(rows, question)\n",
    "\n",
    "                if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self.info_gain(true_rows, false_rows, current_uncertainty)\n",
    "                if gain >= best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_question = question\n",
    "\n",
    "        return best_gain, best_question\n",
    "\n",
    "    def fit(self, rows):\n",
    "        ## Build the tree\n",
    "        ## Rules of recursion:\n",
    "        ## 1) Believe that it works.\n",
    "        ## 2) Start by checking for the base case (no further information gain).\n",
    "        ## 3) Prepare for giant stack traces.\n",
    "        gain, question = self.find_best_split(rows)\n",
    "\n",
    "        if gain == 0:\n",
    "            return Leaf(rows)\n",
    "\n",
    "        true_rows, false_rows = self.partition(rows, question)\n",
    "        true_branch = self.fit(true_rows)\n",
    "        false_branch = self.fit(false_rows)\n",
    "\n",
    "        return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "    def print_tree(self, node, spacing=\"\"):\n",
    "        if isinstance(node, Leaf):\n",
    "            print(spacing + \"Predict\", node.predictions)\n",
    "            return\n",
    "\n",
    "        print(spacing + str(node.question))\n",
    "        print(spacing + '--> True:')\n",
    "        self.print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "        print(spacing + '--> False:')\n",
    "        self.print_tree(node.false_branch, spacing + \"  \")\n",
    "\n",
    "    def classify(self, row, node):\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.predictions\n",
    "\n",
    "        if node.question.match(row):\n",
    "            return self.classify(row, node.true_branch)\n",
    "        else:\n",
    "            return self.classify(row, node.false_branch)\n",
    "\n",
    "    def print_leaf(self, counts):\n",
    "        total = sum(counts.values()) * 1.0\n",
    "        probs = {}\n",
    "        for lbl in counts.keys():\n",
    "            probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "\n",
    "        return probs\n",
    "\n",
    "class Question(DecisionTree):\n",
    "    ## This class is used to partition a dataset.\n",
    "    ## This class just records a column number such as 0 for color and\n",
    "    ## a column value such as green. The match method is used to compare \n",
    "    ## the feature value in an example to the feature value stored in the question.\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        val = example[self.column]\n",
    "        if self.isNumeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        ## Print question in a readable format\n",
    "        condition = \"==\"\n",
    "        if self.isNumeric(self.value):\n",
    "            condition = \">=\"\n",
    "\n",
    "        return \"Is %s %s %s?\" % (header[self.column], condition, str(self.value))\n",
    "\n",
    "class Leaf(DecisionTree):\n",
    "    ## Leaf node classifies data.\n",
    "    ## This holds a dictionary of class such as apple -> number of times\n",
    "    ## It appears in the rows from the training data that reach this leaf.\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = self.class_counts(rows)\n",
    "\n",
    "class Decision_Node(DecisionTree):\n",
    "    ## Decision nodes ask a question\n",
    "    ## This holds a reference to the question, and to the two child nodes.\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is composed by multiple decision trees. We first divide the training data into M subsets. From the M subsets, we can build M decision trees. Then we can shuffle the original training data and divide it into another M subsets. As a result, another M subsets are randomly generated and we can build another M decision trees again. The process is repeted again and again until it reach some pre-defined threshold. For simplicity, I created a variable, n_replacement, as the threshold. The process will end when it reaches the n_replacement. (Process repeted for n_replacement times)\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "Dataset = \n",
    "          \n",
    "    [fA1 fB1 fC1 C1]\n",
    "\n",
    "    [fA2 fB2 fC2 C2]\n",
    "\n",
    "        ...         \n",
    "\n",
    "    [fAN fBN fCN CN]\n",
    "\n",
    "\n",
    "fit:\n",
    "    Empty list forest is created\n",
    "    \n",
    "    Shuffle the data set\n",
    "    \n",
    "    Process = 1\n",
    "\n",
    "        Create M random subsets S_1...S_M\n",
    "\n",
    "        Create M decision trees using the M subsets\n",
    "        \n",
    "        Append M decision trees to forest\n",
    "\n",
    "    Process = 2\n",
    "    \n",
    "        Recreate another M random subsets S_1...S_M\n",
    "\n",
    "        Create another M decision trees using the new M subsets\n",
    "        \n",
    "        Append M decision trees to forest\n",
    "        \n",
    "    Repete until Process reach n_replacement\n",
    "\n",
    "    return forest\n",
    "\n",
    "    if n_replacement = 5, 5*M decision trees should be stored in forest\n",
    "\n",
    "classify: \n",
    "\n",
    "    Loop through forest.\n",
    "    \n",
    "    Each tree in the forest makes prediction. \n",
    "    \n",
    "    If 5*M decision trees are stored in the forest, there should be 5*M predicted values.\n",
    "\n",
    "    Choose the highest confidence result (value that appear most in the resultset). \n",
    "\n",
    "    E.g 80% of trees predict that the result should be class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## from DecisionTree import DecisionTree, Question, Leaf, Decision_Node\n",
    "class RandomForest:\n",
    "    def __init__(self, n_subsets=1, n_replacement=5):\n",
    "        self.innermodel = DecisionTree()\n",
    "        self.n_subsets = n_subsets\n",
    "        self.n_replacement = n_replacement\n",
    "\n",
    "    def generate_subset(self, group_number, data):\n",
    "        subset = []\n",
    "        i = 0\n",
    "        while i < len(data):\n",
    "            if i % self.n_subsets == group_number:\n",
    "                subset.append(data[i])\n",
    "            i += 1\n",
    "\n",
    "        return subset\n",
    "\n",
    "    def fit(self, data):\n",
    "        ## save the original data\n",
    "        self.dataset = data\n",
    "        forest = [] ## use to save the tree of each subset\n",
    "\n",
    "        i = 0\n",
    "        while i < self.n_replacement: \n",
    "            random.shuffle(data)\t\t\n",
    "            ## split data into subsets and save to a list\n",
    "            subsets = []\n",
    "            j = 0\n",
    "            while j < self.n_subsets:\n",
    "                subset = self.generate_subset(j, data)\n",
    "                subsets.append(subset)\n",
    "                j += 1\n",
    "\n",
    "            for subset in subsets:\n",
    "                tree = self.innermodel.fit(subset)\n",
    "                forest.append(tree)\t\n",
    "            i += 1\n",
    "\n",
    "        return forest\n",
    "\n",
    "    def classify(self, row, forest):\n",
    "        results = []\n",
    "        confidence = 0\n",
    "        for node in forest:\n",
    "            result = self.innermodel.classify(row, node)\n",
    "            for key in result:\n",
    "                if len(result) == 1:\n",
    "                    results.append(key)\n",
    "                else:\n",
    "                    results.append(max(result, key=result.get))\n",
    "                    break\n",
    "\n",
    "        classification = max(set(results), key=results.count)\n",
    "        confidence = round(results.count(max(set(results), key=results.count)) / len(results)*100, 2)\n",
    "        return [classification, confidence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine finds the best hyperplane to separate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Support Vector Machine\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "style.use('ggplot')\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1:'r', -1:'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "            \n",
    "    def fit(self, data):\n",
    "        ## data = {class1: np.array([x1, y1, z1], [x2, y2, z2], ..., [xn, yn, zn]),\n",
    "        ##         class2: np.array([x1, y1, z1], [x2, y2, z2], ..., [xn, yn, zn])}\n",
    "        self.data = data\n",
    "        ## {||W||: [W, b]}\n",
    "        opt_dict = {}\n",
    "        transforms = [[1,1], [-1,1],[-1,-1],[1,-1]]\n",
    "        \n",
    "        all_data = []\n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feature in featureset:\n",
    "                    all_data.append(feature)\n",
    "                \n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "        \n",
    "        ## support vectors yi((Xi dot W) + b) = 1\n",
    "        step_size = [self.max_feature_value * 0.1,\n",
    "                    self.max_feature_value * 0.01,\n",
    "                    self.max_feature_value * 0.001]\n",
    "        ## step size is the point of expense\n",
    "        \n",
    "        b_range_multiple = 5\n",
    "        b_multiple = 5\n",
    "        latest_optimum = self.max_feature_value * 10\n",
    "        \n",
    "        for step in step_sizes:\n",
    "            W = np.array([latest_optimum, latest_optimum])\n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1 * (self.max_feature_value * b_range_multiple),\n",
    "                                  self.max_feature_value * b_range_multiple,\n",
    "                                  step * b_range_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        W_t = W * transformation\n",
    "                        found_option = True\n",
    "                        ## Weakest link in the SVM fundamentally\n",
    "                        ## SMO attempts to fix this a bit\n",
    "                        ## yi((Xi dot W) + b) >= 1\n",
    "                        for i in self.data:\n",
    "                            for Xi in self.data[i]:\n",
    "                                yi = i\n",
    "                                if not yi*(np.dot(W_t, Xi) + b) >= 1:\n",
    "                                    found_option = False\n",
    "                        \n",
    "                        if found_option:\n",
    "                            opt_dict[np.linalg.norm(W_t)] = [W_t, b]\n",
    "                            \n",
    "                if W[0] < 0:\n",
    "                    optimized = True\n",
    "                    print('Optimized a step.')\n",
    "                else:\n",
    "                    W = W - step\n",
    "                    \n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            opt_choice = opt_dict[norms[0]]\n",
    "            self.W = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0] + step * 2\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ## sign((X dot W) + b)\n",
    "        classification = np.sign(np.dot(np.array(data), self.W) + self.b)\n",
    "        return classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a labelled dataset, predict the class of new data point by the voting result of the nearest k data point.\n",
    "\n",
    "How to determine the distance? We use euclidean distance, which is the square root of the summation of the square of feature differences. \n",
    "\n",
    "For example, in a 5-dimentional space, point A is in the training data and it belongs to classA with a feature vector = [x1, x2, x3, x4, x5].\n",
    "\n",
    "Given an unknown data point with feature vector = [f1, f2, f3, f4, f5]\n",
    "\n",
    "euclidean distance = sqrt(summation((xi - fi)^2))\n",
    "\n",
    "After computing all the euclidean distance between new data point and training data. The nearest k training data will be chosen into the voting group!\n",
    "\n",
    "Predict value = the most common result among the voting group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## k-Nearest Neighbors\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "class KNearestNeighbors:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def predict(self, predict):\n",
    "        \n",
    "        if len(data) < self.k:\n",
    "            warnings.warn('Data is less than total voting group!')\n",
    "\n",
    "        distances = []\n",
    "        ## for given data (features and labels/group)\n",
    "        for group in self.data:\n",
    "            for features in self.data[group]:\n",
    "                ## compute the euclidean distance\n",
    "                euclidean_distance = np.linalg.norm(np.array(features)-np.array(predict))\n",
    "                distances.append([euclidean_distance, group])\n",
    "\n",
    "        ## for the nearest k data points\n",
    "        votes = [i[1] for i in sorted(distances)[:self.k]]\n",
    "        ## choose the most common group among votes\n",
    "        result = Counter(votes).most_common(1)[0][0]\n",
    "        confidence = Counter(votes).most_common(1)[0][1] / self.k\n",
    "\n",
    "        return result, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike supervised learning, labels are not needed in unsupervised learning. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn from the data.\n",
    "\n",
    "Unsupervised Learning problems can be grouped into clustering and association problems.\n",
    "\n",
    "Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\n",
    "\n",
    "Association:  An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y.\n",
    "\n",
    "The following are the examples of unsupervised learning algorithms:\n",
    "1. k-means for clustering.\n",
    "2. Mean shift for clustering.\n",
    "3. Apriori algorithm for association rule learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## k-Means\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k=2, tol=0.001, max_iteration=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iteration = max_iteration\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = {}\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "        \n",
    "        for i in range(self.max_iteration):\n",
    "            self.classifications = {}\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "                \n",
    "            for featureset in X:\n",
    "                distances = [np.linalg.norm(data - self.centroids[centrold]) for centroid in self.centrolds]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "            \n",
    "            previous_centroid = dict(self.centroids)\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                \n",
    "            optimized = True\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid - original_centroid)/ original_centroid*100.0) > self.tol:\n",
    "                    optimized = False\n",
    "            \n",
    "            if optimized:\n",
    "                break\n",
    "    \n",
    "    def predict(self, data):\n",
    "        distances = [np.linalg.norm(data - self.centroids[centrold]) for centroid in self.centrolds]\n",
    "        classification = distances.index(min(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Mean Shift\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "class MeanShift:\n",
    "    def __init__(self, radius=None, radius_norm_step=100):\n",
    "        self.radius = radius\n",
    "        self.radius_norm_step = radius_norm_step\n",
    "        \n",
    "    def fit(self, X):\n",
    "        if self.radius == None:\n",
    "            all_data_centroid = np.average(X, axis=0)\n",
    "            all_data_norm = np.linalg.norm(all_data_centroid)\n",
    "            self.radius = all_data_norm / self.radius_norm_step\n",
    "        \n",
    "        centroids = {}\n",
    "        for i in range(len(data)):\n",
    "            centroids[i] = data[i]\n",
    "            \n",
    "        weights = [i for i in range(self.radius_norm_step)][::-1]\n",
    "        \n",
    "        while True:\n",
    "            new_centroids = []\n",
    "            for i in centroids:\n",
    "                in_bandwidth = []\n",
    "                centroid = centroids[i]\n",
    "                for featureset in X:\n",
    "                    distance = np.linalg.norm(featureset-centroid)\n",
    "                    if distance == 0:\n",
    "                        distance = 0.000000000000000001\n",
    "                    \n",
    "                    weight_index = int(distance/self.radius)\n",
    "                    if weight_index > self.radius_norm_step - 1:\n",
    "                        weight_index = self.radius_norm_step - 1\n",
    "                    \n",
    "                    to_add = (weight[weight_index]**2) * [featureset]\n",
    "                    in_bandwidth += to_add\n",
    "                    \n",
    "                new_centroid = np.average(in_bandwidth, axis=0)\n",
    "                new_centroids.append(tuple(new_centroid))\n",
    "            \n",
    "            uniques = sorted(list(set(new_centroids)))\n",
    "            to_pop = []\n",
    "            for i in uniques:\n",
    "                if i in to_pop:\n",
    "                    pass\n",
    "                for ii in uniques:\n",
    "                    if i == ii:\n",
    "                        pass\n",
    "                    elif np.linalg.norm(np.array(i)-np.array(ii)) <= self.radius and ii not in to_pop:\n",
    "                        to_pop.append(ii)\n",
    "                        \n",
    "            for i in to_pop:\n",
    "                uniques.remove(i)\n",
    "            \n",
    "            prev_centroids = dict(centroids)\n",
    "            centroids = {}\n",
    "            for i in range(len(uniques)):\n",
    "                centroids[i] = np.array(uniques[i])\n",
    "                \n",
    "            optimized = True\n",
    "            for i in centroids:\n",
    "                if not np.array_equal(centroids[i], prev_centroids[i]):\n",
    "                    optimized = False\n",
    "                if not optimized:\n",
    "                    break\n",
    "            \n",
    "            if optimized:\n",
    "                break\n",
    "        \n",
    "        self.centroids = centroids\n",
    "        self.classifications = {}\n",
    "        \n",
    "        for i in range(len(self.centroids)):\n",
    "            self.classifications[i] = []\n",
    "            \n",
    "        for featureset in X:\n",
    "            distances = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = distances.index(min(distances))\n",
    "            self.classifications[classification].append(featureset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = [sqrt(np.linalg.norm(featureset-self.centroids[centroid])) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances)) \n",
    "        return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori algorithm is an algorithm that finds out the association rules (X --> Y).\n",
    "\n",
    "X is the antecedent and Y is the consequent.\n",
    "\n",
    "There are three metrics for assiciation rules:\n",
    "\n",
    "Support(X --> Y): P(X,Y)\n",
    "\n",
    "Support shows the statistical significance. \n",
    "\n",
    "Confidence(X --> Y): P(Y|X) = P(X,Y)/P(X)\n",
    "\n",
    "Confidence is a conditional probability of Y given X. \n",
    "    \n",
    "Lift(X --> Y): P(X,Y)/(P(X)P(Y)) or P(Y|X)/P(Y)\n",
    "\n",
    "Lift shows whether X and Y are dependent. If L is greater than 1, X and Y are dependent and X makes Y more likely. If L is smaller than 1, X and Y are dependent and X makes Y less likely. If L = 1, X and Y are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Apriori algorithm\n",
    "import sys, operator\n",
    "from collections import defaultdict\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# transaction_data = [['milk', 'bananas', 'chocolate'],\n",
    "#                     ['milk', 'chocolate'],\n",
    "#                     ['milk', 'bananas'],\n",
    "#                     ['chocolate'],\n",
    "#                     ['chocolate'],\n",
    "#                     ['milk', 'chocolate']]\n",
    "\n",
    "class AprioriAlgorithm:\n",
    "    def __init__(self, minSupport=0.15, minConfidence=0.6):\n",
    "        self.minSupport = minSupport\n",
    "        self.minConfidence = minConfidence\n",
    "\n",
    "    def generate_itemSet_transactionList(self, dataset):\n",
    "        itemSet = set()\n",
    "        transactionList = list()\n",
    "        for data in dataset:\n",
    "            transaction = frozenset(data)\n",
    "            transactionList.append(transaction)\n",
    "            for item in transaction:\n",
    "                itemSet.add(frozenset([item]))\n",
    "\n",
    "        return itemSet, transactionList\n",
    "\n",
    "    def getItemsWithMinSupport(self, itemSet, transactionList, freqSet):\n",
    "        resultSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "            for transaction in transactionList:\n",
    "                if item.issubset(transaction):\n",
    "                    self.freqSet[item] += 1\n",
    "                    localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "            support = float(count)/len(transactionList)\n",
    "            if support >= self.minSupport:\n",
    "                resultSet.add(item)\n",
    "\n",
    "        return resultSet\n",
    "\n",
    "    def joinSet(self, itemSet, length):\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "    def subsets(self, arr):\n",
    "        return chain(*[combinations(arr, i+1) for i, a in enumerate(arr)])\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.itemSet, self.transactionList = self.generate_itemSet_transactionList(data)\n",
    "        self.freqSet = defaultdict(int)\n",
    "        largeSet = dict() ## stores (keys = n - itemSets, value = support)\n",
    "        assoRules = dict()\n",
    "\n",
    "        itemWithMinSupport = self.getItemsWithMinSupport(self.itemSet, self.transactionList, self.freqSet)\n",
    "\n",
    "        currentLSet = itemWithMinSupport\n",
    "        k = 2\n",
    "        while(currentLSet != set([])):\n",
    "            largeSet[k-1] = currentLSet\n",
    "            currentLSet = self.joinSet(currentLSet, k)\n",
    "            currentCSet = self.getItemsWithMinSupport(currentLSet, self.transactionList, self.freqSet)\n",
    "            k += 1\n",
    "\n",
    "        RetItems = []\n",
    "        for key, value in largeSet.items():\n",
    "            RetItems.extend([(tuple(item), self.getSupport(item)) for item in value])\n",
    "\n",
    "        RetRules = []\n",
    "        for key, value in largeSet.items():\n",
    "            for item in value:\n",
    "                _subsets = map(frozenset, [x for x in self.subsets(item)])\n",
    "                for element in _subsets:\n",
    "                    remain = item.difference(element)\n",
    "                    if len(remain) > 0:\n",
    "                        confidence = self.getSupport(item) / self.getSupport(element)\n",
    "                        if confidence >= self.minConfidence:\n",
    "                            RetRules.append(((tuple(element), tuple(remain)), confidence))\n",
    "\n",
    "        return RetItems, RetRules\n",
    "\n",
    "    def getSupport(self, item):\n",
    "        return float(self.freqSet[item]) / len(self.transactionList)\n",
    "\n",
    "    def printResults(self, items, rules):\n",
    "        for item, support in sorted(items, key=operator.itemgetter(1)):\n",
    "            print(\"item: %s , %.3f\" % (str(item), support))\n",
    "\n",
    "        print(\"\\n------------------ RULES: \")\n",
    "        for rule, confidence in sorted(rules, key=operator.itemgetter(1)):\n",
    "            pre, post = rule\n",
    "            print(\"Rule: %s ==> %s , %.3f\" % (str(pre), str(post), confidence))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
